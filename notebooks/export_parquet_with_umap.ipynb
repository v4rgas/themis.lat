{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0bbde778",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/victor/Documentos/Coding/hack-25/notebooks/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/tmp/ipykernel_2336067/1569871205.py:5: DtypeWarning: Columns (27,52,63,64,87,97,101) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(\"downloads/lic_2020-1.csv\", encoding=\"latin-1\", sep=\";\")\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from umap import UMAP\n",
    "\n",
    "# Load CSV with latin-1 encoding and semicolon separator\n",
    "df = pd.read_csv(\"downloads/lic_2020-1.csv\", encoding=\"latin-1\", sep=\";\")\n",
    "\n",
    "# Identify numeric columns (excluding CodigoExterno)\n",
    "# Convert columns to numeric where possible, handling comma decimal separators\n",
    "numeric_columns = []\n",
    "for col in df.columns:\n",
    "    if col in (\n",
    "        \"CodigoExterno\",\n",
    "        \"Codigo\",\n",
    "        \"CodigoEstado\",\n",
    "        \"EstadoEtapas\",\n",
    "        \"CodigoUnidad\",\n",
    "        \"Informada\",\n",
    "        \"EsBaseTipo\",\n",
    "        \"ValorTiempoRenovacion\",\n",
    "        \"EsRenovable\",\n",
    "        \"Codigoitem\",\n",
    "        \"CodigoProductoONU\",\n",
    "        \"CodigoSucursalProveedor\",\n",
    "        \"Correlativo\",\n",
    "    ):\n",
    "        continue\n",
    "    # Try to convert to numeric, handling comma decimal separators\n",
    "    # Replace comma with dot for decimal separator\n",
    "    test_series = df[col].astype(str).str.replace(\",\", \".\", regex=False)\n",
    "    numeric_series = pd.to_numeric(test_series, errors=\"coerce\")\n",
    "    # Check if column is numeric (has valid numeric values and not all NaN)\n",
    "    if numeric_series.notna().any():\n",
    "        # Check if the column is actually numeric (most values are numeric)\n",
    "        non_null_count = numeric_series.notna().sum()\n",
    "        total_count = len(numeric_series)\n",
    "        # Consider it numeric if at least 50% of values are numeric\n",
    "        if non_null_count / total_count >= 0.5:\n",
    "            numeric_columns.append(col)\n",
    "\n",
    "# Prepare numeric data for UMAP\n",
    "numeric_data = df[numeric_columns].copy()\n",
    "# Convert to numeric, handling comma decimal separators\n",
    "for col in numeric_columns:\n",
    "    # Replace comma with dot for decimal separator, then convert to numeric\n",
    "    numeric_data[col] = numeric_data[col].astype(str).str.replace(\",\", \".\", regex=False)\n",
    "    numeric_data[col] = pd.to_numeric(numeric_data[col], errors=\"coerce\")\n",
    "# Fill missing values with 0 (or could use median/mean)\n",
    "numeric_data = numeric_data.fillna(0)\n",
    "# Ensure all values are float (not object/string)\n",
    "numeric_data = numeric_data.astype(float).drop_duplicates()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "5b9df395",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('Valor Total Ofertado', 62251),\n",
       " ('MontoUnitarioOferta', 43954),\n",
       " ('MontoLineaAdjudica', 19674),\n",
       " ('CodigoProveedor', 11895),\n",
       " ('Monto Estimado Adjudicado', 6857),\n",
       " ('MontoEstimado', 3463),\n",
       " ('NumeroAprobacion', 2047),\n",
       " ('CantidadAdjudicada', 1306),\n",
       " ('Cantidad Ofertada', 1183),\n",
       " ('Cantidad', 1181),\n",
       " ('CodigoOrganismo', 724),\n",
       " ('CantidadReclamos', 407),\n",
       " ('TiempoDuracionContrato', 89),\n",
       " ('NumeroOferentes', 62),\n",
       " ('FechaTiempoEvaluacion', 37),\n",
       " ('CodigoEstadoLicitacion', 10),\n",
       " ('FechasUsuario', 8),\n",
       " ('TipoAprobacion', 7),\n",
       " ('UnidadTiempoDuracionContrato', 5),\n",
       " ('Estimacion', 4),\n",
       " ('TipoPago', 4),\n",
       " ('EstadoCS', 3),\n",
       " ('Contrato', 3),\n",
       " ('UnidadTiempoContratoLicitacion', 3),\n",
       " ('CodigoTipo', 2),\n",
       " ('TipoConvocatoria', 2),\n",
       " ('Etapas', 2),\n",
       " ('TomaRazon', 2),\n",
       " ('EstadoPublicidadOfertas', 2),\n",
       " ('Obras', 2),\n",
       " ('VisibilidadMonto', 2),\n",
       " ('SubContratacion', 2),\n",
       " ('ExtensionPlazo', 2)]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "col_uniques = []\n",
    "for column in numeric_data.columns:\n",
    "    col_uniques.append((column, numeric_data[column].nunique()))\n",
    "\n",
    "sorted(col_uniques, key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2895238b",
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_data_only_awards = numeric_data[numeric_data[\"CantidadAdjudicada\"] > 0].drop_duplicates()\n",
    "df_awards = df.iloc[numeric_data_only_awards.index].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8e2107ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "def format_text_for_embedding(row):\n",
    "    return \"\\n\\n\".join(\n",
    "        [\n",
    "            (row[\"Nombre\"] if not pd.isna(row[\"Nombre\"]) else \"\"),\n",
    "            (row[\"Descripcion\"] if not pd.isna(row[\"Descripcion\"]) else \"\"),\n",
    "            (\n",
    "                row[\"Nombre producto genrico\"]\n",
    "                if not pd.isna(row[\"Nombre producto genrico\"])\n",
    "                else \"\"\n",
    "            ),\n",
    "            (\n",
    "                row[\"Descripcion linea Adquisicion\"]\n",
    "                if not pd.isna(row[\"Descripcion linea Adquisicion\"])\n",
    "                else \"\"\n",
    "            ),\n",
    "            (\n",
    "                row[\"DescripcionProveedor\"]\n",
    "                if not pd.isna(row[\"DescripcionProveedor\"])\n",
    "                else \"\"\n",
    "            ),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "\n",
    "df_awards.loc[:, \"compiled_text\"] = df_awards.apply(format_text_for_embedding, axis=1)\n",
    "df_awards.loc[:, \"supplier_rut\"] = df_awards[\"RutProveedor\"].map(\n",
    "    lambda x: x.split(\"-\")[0].replace(\".\", \"\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85fc8754",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing embeddings for 42874 texts...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Batches: 100%|██████████| 335/335 [00:11<00:00, 28.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text embeddings shape: (42874, 384)\n",
      "Embedding dimension: 384\n"
     ]
    }
   ],
   "source": [
    "# Compute text embeddings using SentenceTransformer\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "\n",
    "# Load the model\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "\n",
    "# Get compiled_text for awards data (matching the indices)\n",
    "texts = df_awards[\"compiled_text\"].fillna(\"\").tolist()\n",
    "\n",
    "# Compute embeddings efficiently in batches\n",
    "print(f\"Computing embeddings for {len(texts)} texts...\")\n",
    "text_embeddings = model.encode(\n",
    "    texts, batch_size=128, show_progress_bar=True, convert_to_numpy=True\n",
    ")\n",
    "\n",
    "print(f\"Text embeddings shape: {text_embeddings.shape}\")\n",
    "print(f\"Embedding dimension: {text_embeddings.shape[1]}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6036c430",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Combined features shape: (42874, 417)\n",
      "  - Text embedding dimension: 384\n",
      "  - Numeric columns dimension: 33\n",
      "  - Total dimension: 417\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/victor/Documentos/Coding/hack-25/notebooks/.venv/lib/python3.12/site-packages/umap/umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.\n",
      "  warn(\n",
      "/home/victor/Documentos/Coding/hack-25/notebooks/.venv/lib/python3.12/site-packages/sklearn/manifold/_spectral_embedding.py:328: UserWarning: Graph is not fully connected, spectral embedding may not work as expected.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "# Concatenate text embeddings with numeric columns\n",
    "# Ensure numeric_data_only_awards is aligned with text_embeddings\n",
    "numeric_array = numeric_data_only_awards.values.astype(np.float32)\n",
    "\n",
    "# Check for and handle infinite values\n",
    "if np.any(np.isinf(numeric_array)):\n",
    "    print(\"Warning: Found infinite values, replacing with NaN\")\n",
    "    numeric_array = np.where(np.isinf(numeric_array), np.nan, numeric_array)\n",
    "\n",
    "# Replace any remaining NaN with 0\n",
    "numeric_array = np.nan_to_num(numeric_array, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "\n",
    "# Concatenate text embeddings (text_embeddings) with numeric columns (numeric_array)\n",
    "# Result: (n_samples, embedding_dim + n_numeric_features)\n",
    "combined_features = np.concatenate([text_embeddings, numeric_array], axis=1)\n",
    "\n",
    "print(f\"Combined features shape: {combined_features.shape}\")\n",
    "print(f\"  - Text embedding dimension: {text_embeddings.shape[1]}\")\n",
    "print(f\"  - Numeric columns dimension: {numeric_array.shape[1]}\")\n",
    "print(f\"  - Total dimension: {combined_features.shape[1]}\")\n",
    "\n",
    "# Check for duplicate rows (can cause issues with nearest neighbor search)\n",
    "# Add tiny random noise to duplicate rows to make them unique\n",
    "unique_rows, unique_indices, inverse_indices = np.unique(\n",
    "    combined_features, axis=0, return_index=True, return_inverse=True\n",
    ")\n",
    "if len(unique_rows) < len(combined_features):\n",
    "    print(\n",
    "        f\"Warning: Found {len(combined_features) - len(unique_rows)} duplicate rows, adding small noise\"\n",
    "    )\n",
    "    # Add very small random noise to make duplicates unique\n",
    "    np.random.seed(42)\n",
    "    noise = np.random.normal(0, 1e-8, combined_features.shape).astype(np.float32)\n",
    "    combined_features = combined_features + noise\n",
    "\n",
    "# Apply Dimensionality Reduction to the combined features\n",
    "reducer = UMAP(n_components=2, random_state=42)\n",
    "umap_embedding = reducer.fit_transform(combined_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "37b9eb9c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2336067/296279619.py:1: DtypeWarning: Columns (20) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_act = pd.read_csv(\"downloads/PUB_EMPRESAS_PJ_2020_A_2024.txt\", sep=\"\\t\")\n"
     ]
    }
   ],
   "source": [
    "df_act = pd.read_csv(\"downloads/PUB_EMPRESAS_PJ_2020_A_2024.txt\", sep=\"\\t\")\n",
    "# Año comercial\tRUT\tDV\tRazón social\tTramo según ventas\n",
    "# Número de trabajadores dependie\tFecha inicio de actividades vige\tFecha término de giro\n",
    "# Fecha primera inscripción de ac\tTipo término de giro\n",
    "# Tipo de contribuyente\tSubtipo de contribuyente\tTramo capital propio positivo\tTramo capital propio negativo\n",
    "# Rubro económico\tSubrubro económico\tActividad económica\n",
    "# Región\tProvincia\tComuna\n",
    "# R_PRESUNTA\tOTROS_REGIMENES\n",
    "df_act.columns = [\n",
    "    \"fiscal_year\",\n",
    "    \"rut\",\n",
    "    \"dv\",\n",
    "    \"company_name\",\n",
    "    \"sales_bracket\",\n",
    "    \"num_employees\",\n",
    "    \"current_activity_start_date\",\n",
    "    \"activity_end_date\",\n",
    "    \"first_registration_date\",\n",
    "    \"activity_end_type\",\n",
    "    \"contributor_type\",\n",
    "    \"contributor_subtype\",\n",
    "    \"positive_equity_bracket\",\n",
    "    \"negative_equity_bracket\",\n",
    "    \"economic_sector\",\n",
    "    \"economic_subsector\",\n",
    "    \"economic_activity\",\n",
    "    \"region\",\n",
    "    \"province\",\n",
    "    \"commune\",\n",
    "    \"presumed_income\",\n",
    "    \"other_regimes\",\n",
    "]\n",
    "df_act.first_registration_date = pd.to_datetime(\n",
    "    df_act.first_registration_date, errors=\"coerce\"\n",
    ")\n",
    "df_act.current_activity_start_date = pd.to_datetime(\n",
    "    df_act.current_activity_start_date, errors=\"coerce\"\n",
    ")\n",
    "df_act.activity_end_date = pd.to_datetime(df_act.activity_end_date, errors=\"coerce\")\n",
    "\n",
    "rut_to_registration_date = {str(d[\"rut\"]): d[\"first_registration_date\"] for d in df_act[[\"rut\", \"first_registration_date\"]].to_dict(orient=\"records\")}\n",
    "df_awards.loc[:, \"first_activity_date\"] = df_awards[\"supplier_rut\"].map(rut_to_registration_date.get)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "9b5a5d6e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: (42874, 56)\n",
      "Numeric columns found: 33\n",
      "Columns: ['CodigoExterno', 'tender_name', 'supplier_name', 'supplier_rut', 'first_activity_date']... (showing first 5)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>CodigoExterno</th>\n",
       "      <th>tender_name</th>\n",
       "      <th>supplier_name</th>\n",
       "      <th>supplier_rut</th>\n",
       "      <th>first_activity_date</th>\n",
       "      <th>FechaCreacion</th>\n",
       "      <th>FechaCierre</th>\n",
       "      <th>FechaInicio</th>\n",
       "      <th>FechaFinal</th>\n",
       "      <th>FechaPubRespuestas</th>\n",
       "      <th>...</th>\n",
       "      <th>Cantidad</th>\n",
       "      <th>CodigoProveedor</th>\n",
       "      <th>Monto Estimado Adjudicado</th>\n",
       "      <th>Cantidad Ofertada</th>\n",
       "      <th>MontoUnitarioOferta</th>\n",
       "      <th>Valor Total Ofertado</th>\n",
       "      <th>CantidadAdjudicada</th>\n",
       "      <th>MontoLineaAdjudica</th>\n",
       "      <th>x</th>\n",
       "      <th>y</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>812030-5-LQ19</td>\n",
       "      <td>SUMINISTRO DE INMUNOGLOBULINA</td>\n",
       "      <td>GRIFOLS CHILE S A</td>\n",
       "      <td>96582310</td>\n",
       "      <td>1993-01-01</td>\n",
       "      <td>2019-01-18</td>\n",
       "      <td>2020-01-31</td>\n",
       "      <td>2020-01-21</td>\n",
       "      <td>2020-01-22</td>\n",
       "      <td>2020-01-24</td>\n",
       "      <td>...</td>\n",
       "      <td>10.0</td>\n",
       "      <td>54808.0</td>\n",
       "      <td>223973946.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>159200.0</td>\n",
       "      <td>1592000.0</td>\n",
       "      <td>10.0</td>\n",
       "      <td>1592000.0</td>\n",
       "      <td>14.278939</td>\n",
       "      <td>17.310768</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>812030-5-LQ19</td>\n",
       "      <td>SUMINISTRO DE INMUNOGLOBULINA</td>\n",
       "      <td>GRIFOLS CHILE S A</td>\n",
       "      <td>96582310</td>\n",
       "      <td>1993-01-01</td>\n",
       "      <td>2019-01-18</td>\n",
       "      <td>2020-01-31</td>\n",
       "      <td>2020-01-21</td>\n",
       "      <td>2020-01-22</td>\n",
       "      <td>2020-01-24</td>\n",
       "      <td>...</td>\n",
       "      <td>60.0</td>\n",
       "      <td>54808.0</td>\n",
       "      <td>223973946.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>322010.0</td>\n",
       "      <td>19320600.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>19320600.0</td>\n",
       "      <td>-4.153696</td>\n",
       "      <td>-7.208119</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2258-58-LE19</td>\n",
       "      <td>Licitacion de BACK UP para laboratorio</td>\n",
       "      <td>BIOMERIEUX CHILE SPA</td>\n",
       "      <td>96659920</td>\n",
       "      <td>1993-01-01</td>\n",
       "      <td>2019-03-11</td>\n",
       "      <td>2020-01-20</td>\n",
       "      <td>2020-01-10</td>\n",
       "      <td>2020-01-15</td>\n",
       "      <td>2020-01-17</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>33139.0</td>\n",
       "      <td>20954265.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1750.0</td>\n",
       "      <td>1750.0</td>\n",
       "      <td>5000.0</td>\n",
       "      <td>8750000.0</td>\n",
       "      <td>-1.487097</td>\n",
       "      <td>-0.097432</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2258-58-LE19</td>\n",
       "      <td>Licitacion de BACK UP para laboratorio</td>\n",
       "      <td>QUORUX CHILE SPA</td>\n",
       "      <td>76131142</td>\n",
       "      <td>2011-01-26</td>\n",
       "      <td>2019-03-11</td>\n",
       "      <td>2020-01-20</td>\n",
       "      <td>2020-01-10</td>\n",
       "      <td>2020-01-15</td>\n",
       "      <td>2020-01-17</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1333599.0</td>\n",
       "      <td>20954265.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>660.0</td>\n",
       "      <td>660.0</td>\n",
       "      <td>2400.0</td>\n",
       "      <td>1584000.0</td>\n",
       "      <td>-1.616803</td>\n",
       "      <td>0.211644</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2258-58-LE19</td>\n",
       "      <td>Licitacion de BACK UP para laboratorio</td>\n",
       "      <td>QUORUX CHILE SPA</td>\n",
       "      <td>76131142</td>\n",
       "      <td>2011-01-26</td>\n",
       "      <td>2019-03-11</td>\n",
       "      <td>2020-01-20</td>\n",
       "      <td>2020-01-10</td>\n",
       "      <td>2020-01-15</td>\n",
       "      <td>2020-01-17</td>\n",
       "      <td>...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1333599.0</td>\n",
       "      <td>20954265.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>1100.0</td>\n",
       "      <td>1100.0</td>\n",
       "      <td>2400.0</td>\n",
       "      <td>2640000.0</td>\n",
       "      <td>-1.575541</td>\n",
       "      <td>0.177008</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 56 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   CodigoExterno                             tender_name  \\\n",
       "0  812030-5-LQ19           SUMINISTRO DE INMUNOGLOBULINA   \n",
       "1  812030-5-LQ19           SUMINISTRO DE INMUNOGLOBULINA   \n",
       "2   2258-58-LE19  Licitacion de BACK UP para laboratorio   \n",
       "3   2258-58-LE19  Licitacion de BACK UP para laboratorio   \n",
       "4   2258-58-LE19  Licitacion de BACK UP para laboratorio   \n",
       "\n",
       "          supplier_name supplier_rut first_activity_date FechaCreacion  \\\n",
       "0     GRIFOLS CHILE S A     96582310          1993-01-01    2019-01-18   \n",
       "1     GRIFOLS CHILE S A     96582310          1993-01-01    2019-01-18   \n",
       "2  BIOMERIEUX CHILE SPA     96659920          1993-01-01    2019-03-11   \n",
       "3      QUORUX CHILE SPA     76131142          2011-01-26    2019-03-11   \n",
       "4      QUORUX CHILE SPA     76131142          2011-01-26    2019-03-11   \n",
       "\n",
       "  FechaCierre FechaInicio FechaFinal FechaPubRespuestas  ... Cantidad  \\\n",
       "0  2020-01-31  2020-01-21 2020-01-22         2020-01-24  ...     10.0   \n",
       "1  2020-01-31  2020-01-21 2020-01-22         2020-01-24  ...     60.0   \n",
       "2  2020-01-20  2020-01-10 2020-01-15         2020-01-17  ...      1.0   \n",
       "3  2020-01-20  2020-01-10 2020-01-15         2020-01-17  ...      1.0   \n",
       "4  2020-01-20  2020-01-10 2020-01-15         2020-01-17  ...      1.0   \n",
       "\n",
       "  CodigoProveedor Monto Estimado Adjudicado Cantidad Ofertada  \\\n",
       "0         54808.0               223973946.0              10.0   \n",
       "1         54808.0               223973946.0              60.0   \n",
       "2         33139.0                20954265.0               1.0   \n",
       "3       1333599.0                20954265.0               1.0   \n",
       "4       1333599.0                20954265.0               1.0   \n",
       "\n",
       "  MontoUnitarioOferta Valor Total Ofertado  CantidadAdjudicada  \\\n",
       "0            159200.0            1592000.0                10.0   \n",
       "1            322010.0           19320600.0                60.0   \n",
       "2              1750.0               1750.0              5000.0   \n",
       "3               660.0                660.0              2400.0   \n",
       "4              1100.0               1100.0              2400.0   \n",
       "\n",
       "  MontoLineaAdjudica          x          y  \n",
       "0          1592000.0  14.278939  17.310768  \n",
       "1         19320600.0  -4.153696  -7.208119  \n",
       "2          8750000.0  -1.487097  -0.097432  \n",
       "3          1584000.0  -1.616803   0.211644  \n",
       "4          2640000.0  -1.575541   0.177008  \n",
       "\n",
       "[5 rows x 56 columns]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create final DataFrame with CodigoExterno, numeric columns, and UMAP x, y\n",
    "result_df = pd.DataFrame()\n",
    "result_df[\"CodigoExterno\"] = df_awards[\"CodigoExterno\"]\n",
    "result_df[\"tender_name\"] = df_awards[\"Nombre\"]\n",
    "result_df[\"supplier_name\"] = df_awards[\"RazonSocialProveedor\"]\n",
    "result_df[\"supplier_rut\"] = df_awards[\"supplier_rut\"]\n",
    "result_df[\"first_activity_date\"] = pd.to_datetime(df_awards[\"first_activity_date\"])\n",
    "\n",
    "for col in df_awards.columns:\n",
    "    if col.startswith(\"Fecha\"):\n",
    "        result_df[col] = pd.to_datetime(df_awards[col])\n",
    "# Add all numeric columns\n",
    "for col in numeric_columns:\n",
    "    result_df[col] = numeric_data_only_awards[col]\n",
    "# Add UMAP x and y columns\n",
    "result_df[\"x\"] = umap_embedding[:, 0]\n",
    "result_df[\"y\"] = umap_embedding[:, 1]\n",
    "result_df.reset_index(drop=True, inplace=True)\n",
    "# Display result\n",
    "print(f\"Shape: {result_df.shape}\")\n",
    "print(f\"Numeric columns found: {len(numeric_columns)}\")\n",
    "print(f\"Columns: {list(result_df.columns[:5])}... (showing first 5)\")\n",
    "result_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "b5a6f370",
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df.to_parquet('downloads/lic_2020-1_umap.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "830f4434",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/victor/Documentos/Coding/hack-25/notebooks/.venv/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "/tmp/ipykernel_2873040/3855942639.py:11: DtypeWarning: Columns (20) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df_act = pd.read_csv(\"downloads/PUB_EMPRESAS_PJ_2020_A_2024.txt\", sep=\"\\t\")\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from glob import glob\n",
    "from typing import Tuple\n",
    "from concurrent.futures import ProcessPoolExecutor, as_completed\n",
    "import multiprocessing as mp\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "df_act = pd.read_csv(\"downloads/PUB_EMPRESAS_PJ_2020_A_2024.txt\", sep=\"\\t\")\n",
    "# Año comercial\tRUT\tDV\tRazón social\tTramo según ventas\n",
    "# Número de trabajadores dependie\tFecha inicio de actividades vige\tFecha término de giro\n",
    "# Fecha primera inscripción de ac\tTipo término de giro\n",
    "# Tipo de contribuyente\tSubtipo de contribuyente\tTramo capital propio positivo\tTramo capital propio negativo\n",
    "# Rubro económico\tSubrubro económico\tActividad económica\n",
    "# Región\tProvincia\tComuna\n",
    "# R_PRESUNTA\tOTROS_REGIMENES\n",
    "df_act.columns = [\n",
    "    \"fiscal_year\",\n",
    "    \"rut\",\n",
    "    \"dv\",\n",
    "    \"company_name\",\n",
    "    \"sales_bracket\",\n",
    "    \"num_employees\",\n",
    "    \"current_activity_start_date\",\n",
    "    \"activity_end_date\",\n",
    "    \"first_registration_date\",\n",
    "    \"activity_end_type\",\n",
    "    \"contributor_type\",\n",
    "    \"contributor_subtype\",\n",
    "    \"positive_equity_bracket\",\n",
    "    \"negative_equity_bracket\",\n",
    "    \"economic_sector\",\n",
    "    \"economic_subsector\",\n",
    "    \"economic_activity\",\n",
    "    \"region\",\n",
    "    \"province\",\n",
    "    \"commune\",\n",
    "    \"presumed_income\",\n",
    "    \"other_regimes\",\n",
    "]\n",
    "# Note: We'll use safe_to_datetime for dates in the processing function\n",
    "# For df_act, we use errors=\"coerce\" and let safe_to_datetime handle invalid dates later\n",
    "df_act.first_registration_date = pd.to_datetime(\n",
    "    df_act.first_registration_date, errors=\"coerce\"\n",
    ")\n",
    "df_act.current_activity_start_date = pd.to_datetime(\n",
    "    df_act.current_activity_start_date, errors=\"coerce\"\n",
    ")\n",
    "df_act.activity_end_date = pd.to_datetime(df_act.activity_end_date, errors=\"coerce\")\n",
    "\n",
    "rut_to_registration_date = {\n",
    "    str(d[\"rut\"]): d[\"first_registration_date\"]\n",
    "    for d in df_act[[\"rut\", \"first_registration_date\"]].to_dict(orient=\"records\")\n",
    "}\n",
    "\n",
    "# Model will be initialized in each worker process\n",
    "_model_cache = None\n",
    "\n",
    "\n",
    "def get_model():\n",
    "    \"\"\"Lazy load model (one per process).\"\"\"\n",
    "    global _model_cache\n",
    "    if _model_cache is None:\n",
    "        _model_cache = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "    return _model_cache\n",
    "\n",
    "\n",
    "def safe_to_datetime(series, fallback_date=\"1900-01-01\"):\n",
    "    \"\"\"\n",
    "    Safely convert a series to datetime, using fallback_date for invalid dates.\n",
    "    Handles out-of-bounds dates and other parsing errors.\n",
    "    Vectorized version for better performance.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        from pandas._libs.tslibs.np_datetime import OutOfBoundsDatetime\n",
    "    except ImportError:\n",
    "        # OutOfBoundsDatetime might not be available in all pandas versions\n",
    "        OutOfBoundsDatetime = ValueError\n",
    "\n",
    "    fallback = pd.Timestamp(fallback_date)\n",
    "    min_date = pd.Timestamp(\"1677-09-21\")\n",
    "    max_date = pd.Timestamp(\"2262-04-11\")\n",
    "\n",
    "    # Vectorized conversion: try to convert entire series at once\n",
    "    try:\n",
    "        result = pd.to_datetime(series, errors=\"coerce\")\n",
    "    except (ValueError, OverflowError, OutOfBoundsDatetime):\n",
    "        # If vectorized conversion fails due to OutOfBoundsDatetime,\n",
    "        # fall back to element-wise for problematic values\n",
    "        # First, try to convert what we can\n",
    "        result = pd.Series(index=series.index, dtype=\"datetime64[ns]\")\n",
    "        # Identify problematic indices by trying to convert each value\n",
    "        problematic_indices = []\n",
    "        for idx, val in series.items():\n",
    "            try:\n",
    "                if pd.isna(val):\n",
    "                    result.loc[idx] = pd.NaT\n",
    "                else:\n",
    "                    parsed = pd.to_datetime(val, errors=\"coerce\")\n",
    "                    result.loc[idx] = parsed\n",
    "            except (ValueError, OverflowError, OutOfBoundsDatetime):\n",
    "                problematic_indices.append(idx)\n",
    "                result.loc[idx] = pd.NaT\n",
    "        \n",
    "        # For problematic indices, use fallback\n",
    "        if problematic_indices:\n",
    "            result.loc[problematic_indices] = fallback\n",
    "\n",
    "    # Replace NaT (invalid/unparseable dates) with fallback\n",
    "    result = result.fillna(fallback)\n",
    "    \n",
    "    # Replace out-of-bounds dates with fallback using vectorized boolean indexing\n",
    "    out_of_bounds = (result < min_date) | (result > max_date)\n",
    "    result.loc[out_of_bounds] = fallback\n",
    "    \n",
    "    return result\n",
    "\n",
    "\n",
    "def format_text_for_embedding(row):\n",
    "    return \"\\n\\n\".join(\n",
    "        [\n",
    "            (row[\"Nombre\"] if not pd.isna(row[\"Nombre\"]) else \"\"),\n",
    "            (row[\"Descripcion\"] if not pd.isna(row[\"Descripcion\"]) else \"\"),\n",
    "            (\n",
    "                row[\"Nombre producto genrico\"]\n",
    "                if not pd.isna(row[\"Nombre producto genrico\"])\n",
    "                else \"\"\n",
    "            ),\n",
    "            (\n",
    "                row[\"Descripcion linea Adquisicion\"]\n",
    "                if not pd.isna(row[\"Descripcion linea Adquisicion\"])\n",
    "                else \"\"\n",
    "            ),\n",
    "            (\n",
    "                row[\"DescripcionProveedor\"]\n",
    "                if not pd.isna(row[\"DescripcionProveedor\"])\n",
    "                else \"\"\n",
    "            ),\n",
    "        ]\n",
    "    )\n",
    "\n",
    "\n",
    "def identify_numeric_columns(df):\n",
    "    \"\"\"Identify numeric columns in a dataframe.\"\"\"\n",
    "    numeric_columns = []\n",
    "    for col in df.columns:\n",
    "        if col in (\n",
    "            \"CodigoExterno\",\n",
    "            \"Codigo\",\n",
    "            \"CodigoEstado\",\n",
    "            \"EstadoEtapas\",\n",
    "            \"CodigoUnidad\",\n",
    "            \"Informada\",\n",
    "            \"EsBaseTipo\",\n",
    "            \"ValorTiempoRenovacion\",\n",
    "            \"EsRenovable\",\n",
    "            \"Codigoitem\",\n",
    "            \"CodigoProductoONU\",\n",
    "            \"CodigoSucursalProveedor\",\n",
    "            \"Correlativo\",\n",
    "        ):\n",
    "            continue\n",
    "        # Try to convert to numeric, handling comma decimal separators\n",
    "        # Replace comma with dot for decimal separator\n",
    "        test_series = df[col].astype(str).str.replace(\",\", \".\", regex=False)\n",
    "        numeric_series = pd.to_numeric(test_series, errors=\"coerce\")\n",
    "        # Check if column is numeric (has valid numeric values and not all NaN)\n",
    "        if numeric_series.notna().any():\n",
    "            # Check if the column is actually numeric (most values are numeric)\n",
    "            non_null_count = numeric_series.notna().sum()\n",
    "            total_count = len(numeric_series)\n",
    "            # Consider it numeric if at least 50% of values are numeric\n",
    "            if non_null_count / total_count >= 0.5:\n",
    "                numeric_columns.append(col)\n",
    "    return numeric_columns\n",
    "\n",
    "\n",
    "def get_all_numeric_columns(csv_files):\n",
    "    \"\"\"Scan all CSV files to identify the union of all numeric columns.\"\"\"\n",
    "    all_numeric_columns = set()\n",
    "    print(\"Scanning all CSV files to identify numeric columns...\")\n",
    "    for file_path in tqdm(csv_files, desc=\"Scanning files\"):\n",
    "        try:\n",
    "            df = pd.read_csv(\n",
    "                file_path, encoding=\"latin-1\", sep=\";\", nrows=1000\n",
    "            )  # Sample first 1000 rows for speed\n",
    "            numeric_cols = identify_numeric_columns(df)\n",
    "            all_numeric_columns.update(numeric_cols)\n",
    "        except Exception as e:\n",
    "            print(f\"Warning: Error scanning {file_path}: {e}\")\n",
    "            continue\n",
    "    # Convert to sorted list for consistency\n",
    "    numeric_columns_list = sorted(list(all_numeric_columns))\n",
    "    print(f\"Found {len(numeric_columns_list)} numeric columns across all files\")\n",
    "    return numeric_columns_list\n",
    "\n",
    "\n",
    "def process_and_embed_one_file(\n",
    "    file_path, numeric_columns: list, rut_to_registration_date: dict\n",
    ") -> Tuple[pd.DataFrame, np.ndarray]:\n",
    "    \"\"\"Process a single file and return result_df and combined_features.\"\"\"\n",
    "    print(f\"Processing file: {file_path}\")\n",
    "    # Load model in this process\n",
    "    model = get_model()\n",
    "    # Load CSV with latin-1 encoding and semicolon separator\n",
    "    df = pd.read_csv(file_path, encoding=\"latin-1\", sep=\";\")\n",
    "\n",
    "    # Prepare numeric data for UMAP\n",
    "    # Ensure all required numeric columns exist, fill missing ones with 0\n",
    "    numeric_data = pd.DataFrame(index=df.index)\n",
    "    for col in numeric_columns:\n",
    "        if col in df.columns:\n",
    "            numeric_data[col] = df[col]\n",
    "        else:\n",
    "            numeric_data[col] = 0\n",
    "\n",
    "    # Convert to numeric, handling comma decimal separators\n",
    "    for col in numeric_columns:\n",
    "        # Replace comma with dot for decimal separator, then convert to numeric\n",
    "        numeric_data[col] = (\n",
    "            numeric_data[col].astype(str).str.replace(\",\", \".\", regex=False)\n",
    "        )\n",
    "        numeric_data[col] = pd.to_numeric(numeric_data[col], errors=\"coerce\")\n",
    "    # Fill missing values with 0 (or could use median/mean)\n",
    "    numeric_data = numeric_data.fillna(0)\n",
    "    # Ensure all values are float (not object/string)\n",
    "    numeric_data = numeric_data.astype(float).drop_duplicates()\n",
    "\n",
    "    # Filter for awards (CantidadAdjudicada > 0)\n",
    "    if \"CantidadAdjudicada\" not in numeric_data.columns:\n",
    "        print(\n",
    "            f\"Warning: CantidadAdjudicada not found in numeric columns for {file_path}, skipping awards filter\"\n",
    "        )\n",
    "        numeric_data_only_awards = numeric_data.drop_duplicates()\n",
    "    else:\n",
    "        numeric_data_only_awards = numeric_data[\n",
    "            numeric_data[\"CantidadAdjudicada\"] > 0\n",
    "        ].drop_duplicates()\n",
    "    df_awards = df.iloc[numeric_data_only_awards.index].copy()\n",
    "\n",
    "    df_awards.loc[:, \"compiled_text\"] = df_awards.apply(\n",
    "        format_text_for_embedding, axis=1\n",
    "    )\n",
    "    df_awards.loc[:, \"supplier_rut\"] = df_awards[\"RutProveedor\"].map(\n",
    "        lambda x: x.split(\"-\")[0].replace(\".\", \"\")\n",
    "    )\n",
    "\n",
    "    # Get compiled_text for awards data (matching the indices)\n",
    "    texts = df_awards[\"compiled_text\"].fillna(\"\").tolist()\n",
    "\n",
    "    # Compute embeddings efficiently in batches\n",
    "    print(f\"Computing embeddings for {len(texts)} texts in {file_path}...\")\n",
    "    text_embeddings = model.encode(\n",
    "        texts, batch_size=16, show_progress_bar=False, convert_to_numpy=True\n",
    "    )\n",
    "\n",
    "    print(f\"Text embeddings shape: {text_embeddings.shape}\")\n",
    "    print(f\"Embedding dimension: {text_embeddings.shape[1]}\")\n",
    "\n",
    "    # Concatenate text embeddings with numeric columns\n",
    "    # Ensure numeric_data_only_awards is aligned with text_embeddings\n",
    "    numeric_array = numeric_data_only_awards.values.astype(np.float32)\n",
    "\n",
    "    # Check for and handle infinite values\n",
    "    if np.any(np.isinf(numeric_array)):\n",
    "        print(\"Warning: Found infinite values, replacing with NaN\")\n",
    "        numeric_array = np.where(np.isinf(numeric_array), np.nan, numeric_array)\n",
    "\n",
    "    # Replace any remaining NaN with 0\n",
    "    numeric_array = np.nan_to_num(numeric_array, nan=0.0, posinf=0.0, neginf=0.0)\n",
    "\n",
    "    # Concatenate text embeddings (text_embeddings) with numeric columns (numeric_array)\n",
    "    # Result: (n_samples, embedding_dim + n_numeric_features)\n",
    "    combined_features = np.concatenate([text_embeddings, numeric_array], axis=1)\n",
    "\n",
    "    print(f\"Combined features shape: {combined_features.shape}\")\n",
    "    print(f\"  - Text embedding dimension: {text_embeddings.shape[1]}\")\n",
    "    print(f\"  - Numeric columns dimension: {numeric_array.shape[1]}\")\n",
    "    print(f\"  - Total dimension: {combined_features.shape[1]}\")\n",
    "\n",
    "    df_awards.loc[:, \"first_activity_date\"] = df_awards[\"supplier_rut\"].map(\n",
    "        rut_to_registration_date.get\n",
    "    )\n",
    "\n",
    "    # Create final DataFrame with CodigoExterno, numeric columns (without UMAP x, y)\n",
    "    result_df = pd.DataFrame()\n",
    "    result_df[\"CodigoExterno\"] = df_awards[\"CodigoExterno\"]\n",
    "    result_df[\"tender_name\"] = df_awards[\"Nombre\"]\n",
    "    result_df[\"supplier_name\"] = df_awards[\"RazonSocialProveedor\"]\n",
    "    result_df[\"supplier_rut\"] = df_awards[\"supplier_rut\"]\n",
    "    result_df[\"first_activity_date\"] = safe_to_datetime(\n",
    "        df_awards[\"first_activity_date\"]\n",
    "    )\n",
    "\n",
    "    for col in df_awards.columns:\n",
    "        if col.startswith(\"Fecha\"):\n",
    "            result_df[col] = safe_to_datetime(df_awards[col])\n",
    "    # Add all numeric columns\n",
    "    for col in numeric_columns:\n",
    "        result_df[col] = numeric_data_only_awards[col]\n",
    "    result_df.reset_index(drop=True, inplace=True)\n",
    "    # Display result\n",
    "    print(f\"Shape: {result_df.shape}\")\n",
    "    print(f\"Numeric columns found: {len(numeric_columns)}\")\n",
    "    print(f\"Columns: {list(result_df.columns[:5])}... (showing first 5)\")\n",
    "\n",
    "    return result_df, combined_features\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9f8d56d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 8 CSV files to process\n",
      "Scanning all CSV files to identify numeric columns...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Scanning files: 100%|██████████| 8/8 [00:00<00:00, 16.09it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 35 numeric columns across all files\n",
      "\n",
      "Processing files and computing embeddings in parallel...\n",
      "Using 8 worker processes\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Processing files:   0%|          | 0/8 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: downloads/lic_2020-1.csv\n",
      "Processing file: downloads/lic_2020-10.csv\n",
      "Processing file: downloads/lic_2020-11.csv\n",
      "Processing file: downloads/lic_2020-12.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2873040/3855942639.py:208: DtypeWarning: Columns (27,52,63,64,87,97,101) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path, encoding=\"latin-1\", sep=\";\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: downloads/lic_2020-2.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2873040/3855942639.py:208: DtypeWarning: Columns (27,52,63,64,87,97,101) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path, encoding=\"latin-1\", sep=\";\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: downloads/lic_2020-3.csv\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2873040/3855942639.py:208: DtypeWarning: Columns (27,52,64,87,97,101) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path, encoding=\"latin-1\", sep=\";\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing embeddings for 28562 texts in downloads/lic_2020-10.csv...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2873040/3855942639.py:208: DtypeWarning: Columns (27,52,59,63,64,65,73,87,94,97,101) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path, encoding=\"latin-1\", sep=\";\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing file: downloads/lic_2020-4.csv\n",
      "Computing embeddings for 42874 texts in downloads/lic_2020-1.csv...\n",
      "Computing embeddings for 30787 texts in downloads/lic_2020-11.csv...\n",
      "Processing file: downloads/lic_2020-5.csv\n",
      "Computing embeddings for 23751 texts in downloads/lic_2020-12.csv...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2873040/3855942639.py:208: DtypeWarning: Columns (27,52,63,64,87,97,101) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path, encoding=\"latin-1\", sep=\";\")\n",
      "/tmp/ipykernel_2873040/3855942639.py:208: DtypeWarning: Columns (27,52,63,64,87,97,101) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path, encoding=\"latin-1\", sep=\";\")\n",
      "/tmp/ipykernel_2873040/3855942639.py:208: DtypeWarning: Columns (27,52,63,64,87,97,101) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path, encoding=\"latin-1\", sep=\";\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing embeddings for 35939 texts in downloads/lic_2020-2.csv...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2873040/3855942639.py:208: DtypeWarning: Columns (27,52,63,64,73,87,94,97,101,102) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  df = pd.read_csv(file_path, encoding=\"latin-1\", sep=\";\")\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Computing embeddings for 32625 texts in downloads/lic_2020-3.csv...\n",
      "Computing embeddings for 23944 texts in downloads/lic_2020-4.csv...\n",
      "Computing embeddings for 24433 texts in downloads/lic_2020-5.csv...\n",
      "Text embeddings shape: (23751, 384)\n",
      "Embedding dimension: 384\n",
      "Combined features shape: (23751, 419)\n",
      "  - Text embedding dimension: 384\n",
      "  - Numeric columns dimension: 35\n",
      "  - Total dimension: 419\n",
      "Shape: (23751, 56)\n",
      "Numeric columns found: 35\n",
      "Columns: ['CodigoExterno', 'tender_name', 'supplier_name', 'supplier_rut', 'first_activity_date']... (showing first 5)\n",
      "Text embeddings shape: (28562, 384)\n",
      "Embedding dimension: 384\n",
      "Combined features shape: (28562, 419)\n",
      "  - Text embedding dimension: 384\n",
      "  - Numeric columns dimension: 35\n",
      "  - Total dimension: 419\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  12%|█▎        | 1/8 [00:52<06:04, 52.12s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape: (28562, 56)\n",
      "Numeric columns found: 35\n",
      "Columns: ['CodigoExterno', 'tender_name', 'supplier_name', 'supplier_rut', 'first_activity_date']... (showing first 5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  25%|██▌       | 2/8 [00:52<02:09, 21.62s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text embeddings shape: (23944, 384)\n",
      "Embedding dimension: 384\n",
      "Combined features shape: (23944, 419)\n",
      "  - Text embedding dimension: 384\n",
      "  - Numeric columns dimension: 35\n",
      "  - Total dimension: 419\n",
      "Shape: (23944, 56)\n",
      "Numeric columns found: 35\n",
      "Columns: ['CodigoExterno', 'tender_name', 'supplier_name', 'supplier_rut', 'first_activity_date']... (showing first 5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  38%|███▊      | 3/8 [00:56<01:09, 13.84s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text embeddings shape: (30787, 384)\n",
      "Embedding dimension: 384\n",
      "Combined features shape: (30787, 419)\n",
      "  - Text embedding dimension: 384\n",
      "  - Numeric columns dimension: 35\n",
      "  - Total dimension: 419\n",
      "Text embeddings shape: (24433, 384)\n",
      "Embedding dimension: 384\n",
      "Combined features shape: (24433, 419)\n",
      "  - Text embedding dimension: 384\n",
      "  - Numeric columns dimension: 35\n",
      "  - Total dimension: 419\n",
      "Shape: (30787, 56)\n",
      "Numeric columns found: 35\n",
      "Columns: ['CodigoExterno', 'tender_name', 'supplier_name', 'supplier_rut', 'first_activity_date']... (showing first 5)\n",
      "Shape: (24433, 56)\n",
      "Numeric columns found: 35\n",
      "Columns: ['CodigoExterno', 'tender_name', 'supplier_name', 'supplier_rut', 'first_activity_date']... (showing first 5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  50%|█████     | 4/8 [00:57<00:34,  8.71s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text embeddings shape: (32625, 384)\n",
      "Embedding dimension: 384\n",
      "Combined features shape: (32625, 419)\n",
      "  - Text embedding dimension: 384\n",
      "  - Numeric columns dimension: 35\n",
      "  - Total dimension: 419\n",
      "Shape: (32625, 56)\n",
      "Numeric columns found: 35\n",
      "Columns: ['CodigoExterno', 'tender_name', 'supplier_name', 'supplier_rut', 'first_activity_date']... (showing first 5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  75%|███████▌  | 6/8 [01:01<00:10,  5.08s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text embeddings shape: (35939, 384)\n",
      "Embedding dimension: 384\n",
      "Combined features shape: (35939, 419)\n",
      "  - Text embedding dimension: 384\n",
      "  - Numeric columns dimension: 35\n",
      "  - Total dimension: 419\n",
      "Shape: (35939, 56)\n",
      "Numeric columns found: 35\n",
      "Columns: ['CodigoExterno', 'tender_name', 'supplier_name', 'supplier_rut', 'first_activity_date']... (showing first 5)\n",
      "Text embeddings shape: (42874, 384)\n",
      "Embedding dimension: 384\n",
      "Combined features shape: (42874, 419)\n",
      "  - Text embedding dimension: 384\n",
      "  - Numeric columns dimension: 35\n",
      "  - Total dimension: 419\n",
      "Shape: (42874, 56)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files:  88%|████████▊ | 7/8 [01:02<00:03,  3.92s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Numeric columns found: 35\n",
      "Columns: ['CodigoExterno', 'tender_name', 'supplier_name', 'supplier_rut', 'first_activity_date']... (showing first 5)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing files: 100%|██████████| 8/8 [01:02<00:00,  7.83s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Concatenating all result DataFrames...\n",
      "Final result DataFrame shape: (242915, 56)\n",
      "\n",
      "Concatenating all combined features...\n",
      "Combined features array shape: (242915, 419)\n",
      "\n",
      "Applying global normalization to numeric features...\n",
      "  - Text embedding columns: 0-383 (keeping as-is)\n",
      "  - Numeric feature columns: 384-418 (normalizing)\n",
      "Global normalization complete. Feature statistics:\n",
      "  - Text embeddings: mean=-0.0024, std=0.0510\n",
      "  - Numeric features: mean=-0.0000, std=1.0000\n",
      "\n",
      "Checking for duplicate rows...\n",
      "Warning: Found 1 duplicate rows, adding small noise\n"
     ]
    }
   ],
   "source": [
    "# Find all CSV files matching the pattern\n",
    "csv_files = sorted(glob(\"downloads/lic_*.csv\"))[:8]\n",
    "if not csv_files:\n",
    "    print(\"No CSV files found matching downloads/lic_*.csv\")\n",
    "    exit(1)\n",
    "\n",
    "print(f\"Found {len(csv_files)} CSV files to process\")\n",
    "\n",
    "# First pass: identify all numeric columns across all files\n",
    "all_numeric_columns = get_all_numeric_columns(csv_files)\n",
    "\n",
    "# Process each file in parallel and collect results\n",
    "all_result_dfs = []\n",
    "all_combined_features = []\n",
    "\n",
    "print(\"\\nProcessing files and computing embeddings in parallel...\")\n",
    "max_workers = min(24, len(csv_files), mp.cpu_count())\n",
    "print(f\"Using {max_workers} worker processes\")\n",
    "\n",
    "with ProcessPoolExecutor(max_workers=max_workers) as executor:\n",
    "    # Submit all tasks\n",
    "    future_to_file = {\n",
    "        executor.submit(\n",
    "            process_and_embed_one_file,\n",
    "            file_path,\n",
    "            all_numeric_columns,\n",
    "            rut_to_registration_date,\n",
    "        ): file_path\n",
    "        for file_path in csv_files\n",
    "    }\n",
    "\n",
    "    # Collect results as they complete\n",
    "    for future in tqdm(\n",
    "        as_completed(future_to_file), total=len(csv_files), desc=\"Processing files\"\n",
    "    ):\n",
    "        file_path = future_to_file[future]\n",
    "        try:\n",
    "            result_df, combined_features = future.result()\n",
    "            all_result_dfs.append(result_df)\n",
    "            all_combined_features.append(combined_features)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {file_path}: {e}\")\n",
    "            import traceback\n",
    "\n",
    "            traceback.print_exc()\n",
    "            continue\n",
    "\n",
    "if not all_result_dfs:\n",
    "    print(\"No files were successfully processed\")\n",
    "    exit(1)\n",
    "\n",
    "# Concatenate all result DataFrames\n",
    "print(\"\\nConcatenating all result DataFrames...\")\n",
    "final_result_df = pd.concat(all_result_dfs, ignore_index=True)\n",
    "print(f\"Final result DataFrame shape: {final_result_df.shape}\")\n",
    "\n",
    "# Concatenate all combined features\n",
    "print(\"\\nConcatenating all combined features...\")\n",
    "all_combined_features_array = np.concatenate(all_combined_features, axis=0)\n",
    "print(f\"Combined features array shape: {all_combined_features_array.shape}\")\n",
    "\n",
    "# Apply global normalization to numeric features only (text embeddings are already normalized)\n",
    "# Split features: text embeddings (first 384 cols) and numeric features (last 33 cols)\n",
    "text_embedding_dim = 384\n",
    "numeric_feature_dim = all_combined_features_array.shape[1] - text_embedding_dim\n",
    "\n",
    "print(f\"\\nApplying global normalization to numeric features...\")\n",
    "print(f\"  - Text embedding columns: 0-{text_embedding_dim-1} (keeping as-is)\")\n",
    "print(f\"  - Numeric feature columns: {text_embedding_dim}-{all_combined_features_array.shape[1]-1} (normalizing)\")\n",
    "\n",
    "# Extract numeric features (last numeric_feature_dim columns)\n",
    "numeric_features = all_combined_features_array[:, text_embedding_dim:]\n",
    "\n",
    "# Normalize numeric features globally\n",
    "scaler = StandardScaler()\n",
    "numeric_features_normalized = scaler.fit_transform(numeric_features).astype(np.float32)\n",
    "\n",
    "# Reconstruct combined features with normalized numeric part\n",
    "all_combined_features_array = np.concatenate(\n",
    "    [all_combined_features_array[:, :text_embedding_dim], numeric_features_normalized], \n",
    "    axis=1\n",
    ")\n",
    "print(f\"Global normalization complete. Feature statistics:\")\n",
    "print(f\"  - Text embeddings: mean={all_combined_features_array[:, :text_embedding_dim].mean():.4f}, std={all_combined_features_array[:, :text_embedding_dim].std():.4f}\")\n",
    "print(f\"  - Numeric features: mean={numeric_features_normalized.mean():.4f}, std={numeric_features_normalized.std():.4f}\")\n",
    "\n",
    "# Check for duplicate rows (can cause issues with nearest neighbor search)\n",
    "# Add tiny random noise to duplicate rows to make them unique\n",
    "print(\"\\nChecking for duplicate rows...\")\n",
    "unique_rows, unique_indices, inverse_indices = np.unique(\n",
    "    all_combined_features_array, axis=0, return_index=True, return_inverse=True\n",
    ")\n",
    "if len(unique_rows) < len(all_combined_features_array):\n",
    "    print(\n",
    "        f\"Warning: Found {len(all_combined_features_array) - len(unique_rows)} duplicate rows, adding small noise\"\n",
    "    )\n",
    "    # Add very small random noise to make duplicates unique\n",
    "    np.random.seed(42)\n",
    "    noise = np.random.normal(0, 1e-8, all_combined_features_array.shape).astype(\n",
    "        np.float32\n",
    "    )\n",
    "    all_combined_features_array = all_combined_features_array + noise\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8e4e55cf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openTSNE import TSNE\n",
    "X_embedded = TSNE(n_components=2, perplexity=15, n_jobs=-1).fit(all_combined_features_array)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d67e8f7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Computing UMAP on all combined features...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/victor/Documentos/Coding/hack-25/notebooks/.venv/lib/python3.12/site-packages/torchdr/distance/faiss.py:104: UserWarning: [TorchDR] WARNING: `faiss-gpu` not installed, using CPU for Faiss computations. This may be slow. For faster performance, install `faiss-gpu`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "bincount only supports 1-d non-negative integral inputs.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[3]\u001b[39m\u001b[32m, line 6\u001b[39m\n\u001b[32m      4\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[33mComputing UMAP on all combined features...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      5\u001b[39m reducer = UMAP(n_components=\u001b[32m2\u001b[39m, random_state=\u001b[32m42\u001b[39m, device=\u001b[33m\"\u001b[39m\u001b[33mcuda\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m6\u001b[39m umap_embedding = \u001b[43mreducer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit_transform\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      7\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfrom_numpy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mall_combined_features_array\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mcuda\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      8\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m# Add UMAP x and y columns to the final result DataFrame\u001b[39;00m\n\u001b[32m     11\u001b[39m final_result_df[\u001b[33m\"\u001b[39m\u001b[33mx\u001b[39m\u001b[33m\"\u001b[39m] = umap_embedding[:, \u001b[32m0\u001b[39m]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documentos/Coding/hack-25/notebooks/.venv/lib/python3.12/site-packages/torchdr/utils/wrappers.py:170\u001b[39m, in \u001b[36mhandle_type.<locals>.decorator_handle_type.<locals>.wrapper\u001b[39m\u001b[34m(self, X, *args, **kwargs)\u001b[39m\n\u001b[32m    159\u001b[39m device = \u001b[38;5;28mself\u001b[39m.device \u001b[38;5;28;01mif\u001b[39;00m set_device \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mauto\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    160\u001b[39m X_, input_backend, input_device = to_torch(\n\u001b[32m    161\u001b[39m     X,\n\u001b[32m    162\u001b[39m     device=device,\n\u001b[32m   (...)\u001b[39m\u001b[32m    168\u001b[39m     **check_array_kwargs,\n\u001b[32m    169\u001b[39m )\n\u001b[32m--> \u001b[39m\u001b[32m170\u001b[39m output = \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    171\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m torch_to_backend(output, backend=input_backend, device=input_device)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documentos/Coding/hack-25/notebooks/.venv/lib/python3.12/site-packages/torchdr/base.py:155\u001b[39m, in \u001b[36mDRModule.fit_transform\u001b[39m\u001b[34m(self, X, y)\u001b[39m\n\u001b[32m    153\u001b[39m         \u001b[38;5;28mself\u001b[39m.embedding_ = embedding_unique[inverse_indices]\n\u001b[32m    154\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m155\u001b[39m         \u001b[38;5;28mself\u001b[39m.embedding_ = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_fit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m=\u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    156\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    157\u001b[39m     \u001b[38;5;28mself\u001b[39m.embedding_ = \u001b[38;5;28mself\u001b[39m._fit_transform(X, y=y)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documentos/Coding/hack-25/notebooks/.venv/lib/python3.12/site-packages/torchdr/neighbor_embedding/base.py:197\u001b[39m, in \u001b[36mNeighborEmbedding._fit_transform\u001b[39m\u001b[34m(self, X, y)\u001b[39m\n\u001b[32m    192\u001b[39m \u001b[38;5;28mself\u001b[39m._check_n_neighbors(X.shape[\u001b[32m0\u001b[39m])\n\u001b[32m    193\u001b[39m \u001b[38;5;28mself\u001b[39m.early_exaggeration_coeff_ = (\n\u001b[32m    194\u001b[39m     \u001b[38;5;28mself\u001b[39m.early_exaggeration_coeff\n\u001b[32m    195\u001b[39m )  \u001b[38;5;66;03m# early_exaggeration_ may change during the optimization\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m197\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_fit_transform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documentos/Coding/hack-25/notebooks/.venv/lib/python3.12/site-packages/torchdr/affinity_matcher.py:231\u001b[39m, in \u001b[36mAffinityMatcher._fit_transform\u001b[39m\u001b[34m(self, X, y)\u001b[39m\n\u001b[32m    227\u001b[39m     \u001b[38;5;28mself\u001b[39m.logger.info(\n\u001b[32m    228\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33m----- Computing the input affinity matrix with \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m.affinity_in.\u001b[34m__class__\u001b[39m.\u001b[34m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m -----\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    229\u001b[39m     )\n\u001b[32m    230\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m.affinity_in, SparseAffinity):\n\u001b[32m--> \u001b[39m\u001b[32m231\u001b[39m     \u001b[38;5;28mself\u001b[39m.affinity_in_, \u001b[38;5;28mself\u001b[39m.NN_indices_ = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43maffinity_in\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    232\u001b[39m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_indices\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\n\u001b[32m    233\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    234\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    235\u001b[39m     \u001b[38;5;28mself\u001b[39m.affinity_in_ = \u001b[38;5;28mself\u001b[39m.affinity_in(X)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documentos/Coding/hack-25/notebooks/.venv/lib/python3.12/site-packages/torchdr/affinity/base.py:325\u001b[39m, in \u001b[36mSparseAffinity.__call__\u001b[39m\u001b[34m(self, X, return_indices, **kwargs)\u001b[39m\n\u001b[32m    323\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m._pre_processed:\n\u001b[32m    324\u001b[39m     X = to_torch(X, device=\u001b[38;5;28mself\u001b[39m.device)\n\u001b[32m--> \u001b[39m\u001b[32m325\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_compute_sparse_affinity\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mreturn_indices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documentos/Coding/hack-25/notebooks/.venv/lib/python3.12/site-packages/torchdr/utils/wrappers.py:248\u001b[39m, in \u001b[36mcompile_if_requested.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    245\u001b[39m     should_compile = kwargs[\u001b[33m\"\u001b[39m\u001b[33mcompile\u001b[39m\u001b[33m\"\u001b[39m]\n\u001b[32m    247\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m should_compile:\n\u001b[32m--> \u001b[39m\u001b[32m248\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    250\u001b[39m \u001b[38;5;66;03m# Create a unique key for the compiled function\u001b[39;00m\n\u001b[32m    251\u001b[39m \u001b[38;5;66;03m# For methods, key on the instance id to recompile for different instances\u001b[39;00m\n\u001b[32m    252\u001b[39m \u001b[38;5;66;03m# For functions, key on the function itself\u001b[39;00m\n\u001b[32m    253\u001b[39m key = (\u001b[38;5;28mid\u001b[39m(\u001b[38;5;28mself\u001b[39m), func) \u001b[38;5;28;01mif\u001b[39;00m is_method \u001b[38;5;28;01melse\u001b[39;00m func\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documentos/Coding/hack-25/notebooks/.venv/lib/python3.12/site-packages/torchdr/affinity/knn_normalized.py:439\u001b[39m, in \u001b[36mUMAPAffinity._compute_sparse_affinity\u001b[39m\u001b[34m(self, X, return_indices, **kwargs)\u001b[39m\n\u001b[32m    437\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.symmetrize:\n\u001b[32m    438\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.sparsity:\n\u001b[32m--> \u001b[39m\u001b[32m439\u001b[39m         affinity_matrix, indices = \u001b[43msym_sparse_op\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    440\u001b[39m \u001b[43m            \u001b[49m\u001b[43maffinity_matrix\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mindices\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43msum_minus_prod\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\n\u001b[32m    441\u001b[39m \u001b[43m        \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    442\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    443\u001b[39m         affinity_matrix = (\n\u001b[32m    444\u001b[39m             affinity_matrix\n\u001b[32m    445\u001b[39m             + matrix_transpose(affinity_matrix)\n\u001b[32m    446\u001b[39m             - affinity_matrix * matrix_transpose(affinity_matrix)\n\u001b[32m    447\u001b[39m         )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documentos/Coding/hack-25/notebooks/.venv/lib/python3.12/site-packages/torchdr/utils/sparse.py:175\u001b[39m, in \u001b[36msym_sparse_op\u001b[39m\u001b[34m(values, indices, mode)\u001b[39m\n\u001b[32m    172\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mUnsupported mode \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmode\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[33m\"\u001b[39m)\n\u001b[32m    174\u001b[39m \u001b[38;5;66;03m# 4) pack back to padded row-wise format\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m175\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mpack_to_rowwise\u001b[49m\u001b[43m(\u001b[49m\u001b[43mi_out\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mj_out\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mv_out\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documentos/Coding/hack-25/notebooks/.venv/lib/python3.12/site-packages/torchdr/utils/sparse.py:110\u001b[39m, in \u001b[36mpack_to_rowwise\u001b[39m\u001b[34m(i_out, j_out, v_out, n)\u001b[39m\n\u001b[32m     87\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mpack_to_rowwise\u001b[39m(\n\u001b[32m     88\u001b[39m     i_out: torch.LongTensor, j_out: torch.LongTensor, v_out: torch.Tensor, n: \u001b[38;5;28mint\u001b[39m\n\u001b[32m     89\u001b[39m ) -> Tuple[torch.Tensor, torch.LongTensor]:\n\u001b[32m     90\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Pack flat entries back into padded row-wise format.\u001b[39;00m\n\u001b[32m     91\u001b[39m \n\u001b[32m     92\u001b[39m \u001b[33;03m    Parameters\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    108\u001b[39m \u001b[33;03m        Padded indices tensor of shape (n, k_out), with -1 for unused slots.\u001b[39;00m\n\u001b[32m    109\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m110\u001b[39m     counts = \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbincount\u001b[49m\u001b[43m(\u001b[49m\u001b[43mi_out\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mminlength\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    111\u001b[39m     max_k_out = counts.max()\n\u001b[32m    113\u001b[39m     values_out = torch.zeros((n, max_k_out), dtype=v_out.dtype, device=v_out.device)\n",
      "\u001b[31mRuntimeError\u001b[39m: bincount only supports 1-d non-negative integral inputs."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Apply Dimensionality Reduction to the combined features (once for all data)\n",
    "print(\"\\nComputing UMAP on all combined features...\")\n",
    "reducer = UMAP(n_components=2, random_state=42, device=\"cuda\")\n",
    "reduced_embedding = reducer.fit_transform(\n",
    "    torch.from_numpy(all_combined_features_array).to(device=\"cuda\")\n",
    ")\n",
    "\n",
    "# Add UMAP x and y columns to the final result DataFrame\n",
    "final_result_df[\"x\"] = reduced_embedding[:, 0]\n",
    "final_result_df[\"y\"] = reduced_embedding[:, 1]\n",
    "\n",
    "# Save final output\n",
    "output_path = \"downloads/all_months_umap_gpu.parquet\"\n",
    "print(f\"\\nSaving final result to {output_path}...\")\n",
    "final_result_df.to_parquet(output_path)\n",
    "\n",
    "print(f\"\\nCompleted! Final dataset shape: {final_result_df.shape}\")\n",
    "print(f\"Columns: {list(final_result_df.columns)}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
